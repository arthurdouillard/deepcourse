<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DeepCourse</title>
    <link>/</link>
    <description>Recent content on DeepCourse</description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Convolutional Neural Network</title>
      <link>/cnn/</link>
      <pubDate>Mon, 18 Jun 1956 00:00:00 +0000</pubDate>
      
      <guid>/cnn/</guid>
      <description>Before the lecture  Quick glance at the wikipedia page of convolution kernels  The lecture 
After the lecture Emblematic papers about new architectures to read:
 VGG, Simonyan and Zisserman 2014 Inception, Szegedy et al. CVPR 2015 ResNet, He et al. ECCV 2016 EfficientNet, Tan et al. ICML 2019  Other papers worth reading:
 Dropout, Srivastava et al. JMLR 2014 Batch Normalization, Ioffe et al. ICML 2015  Some blog posts:</description>
    </item>
    
    <item>
      <title>Deep Neural Network</title>
      <link>/dnn/</link>
      <pubDate>Mon, 18 Jun 1956 00:00:00 +0000</pubDate>
      
      <guid>/dnn/</guid>
      <description>Before the lecture If you like videos, you can watch (as much as you can) the videos of 3Blue1Brown on Neural Networks:
  if you need to brush up your skills in linear algebra: Hui&amp;rsquo;s medium blog post  The lecture 
After the lecture   Ruder&amp;rsquo;s overview of gradient-descent based optimizer (SGD, momentum, Adam, etc.)
  try re-deriving by yourself, on a sheet of paper, all the maths that were covered</description>
    </item>
    
    <item>
      <title>Deep Neural Network</title>
      <link>/tmp1234/</link>
      <pubDate>Mon, 18 Jun 1956 00:00:00 +0000</pubDate>
      
      <guid>/tmp1234/</guid>
      <description>This lesson has more math than usual, but it&amp;rsquo;s important that we don&amp;rsquo;t shy away the theory behind deep learning. It will help us build a strong understanding of the algorithms we will code in later sections. If you feel overwhelmed by the math here, please see this review first.
We are talking of neural networks, then what is a neuron? Certainly not the kind that is in your brain. While biologically inspired, artificial neurons are very different from the real counterpart, thus for now put aside any comparison with brains.</description>
    </item>
    
    <item>
      <title>Generative Models</title>
      <link>/generative/</link>
      <pubDate>Mon, 18 Jun 1956 00:00:00 +0000</pubDate>
      
      <guid>/generative/</guid>
      <description>The lecture 
After the lecture   Blog on auto-encoder, VAE, and beta-VAE
  Training tricks for GAN
  Youtube video on the demonstration of VAE
  Stanford&amp;rsquo;s course on generative models
  Tutorial on Style Transfer
  Paper on why Style Transfer works better with VGG than ResNet
  Lilian Weng on Normalizing Flows
  Generate yourself amazing artworks with VQGAN+CLIP [Google Colab]</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>/introduction/</link>
      <pubDate>Mon, 18 Jun 1956 00:00:00 +0000</pubDate>
      
      <guid>/introduction/</guid>
      <description>Welcome to this course on Deep Learning for Computer Vision!
This course will teach you the fundamentals of Deep Learning applied to images. Each topic will be covered under three different types of materials:
 Lessons, such as this one, accompanied by slides and small quizzes between chapters; Practices, with Google Colab notebooks where we will code actual algorithms; And, quizzes, where you will test your recall on the important notions.</description>
    </item>
    
    <item>
      <title>Less Data</title>
      <link>/lessdata/</link>
      <pubDate>Mon, 18 Jun 1956 00:00:00 +0000</pubDate>
      
      <guid>/lessdata/</guid>
      <description>The lecture 
After the lecture Emblematic papers about new architectures to read:
Blog posts:
 On the Triplet Network Self-Supervised Learning Twitter thread on recent advances in self-supervision  Papers on few-shot learning:
 Triplet Network Siamese Network for one-shot learning  Papers on self-supervision:
 SimCRL MoCo v1 BYOL Barlow Twins  Papers on domain adaptation:
 DANN and its Gradient Reversal Layer AdaptSegNet  Paper on weak-supervision:</description>
    </item>
    
    <item>
      <title>Multiple Labels</title>
      <link>/multilabels/</link>
      <pubDate>Mon, 18 Jun 1956 00:00:00 +0000</pubDate>
      
      <guid>/multilabels/</guid>
      <description>The lecture 
After the lecture Emblematic papers about new architectures to read:
Series of blog posts:
 Selective Search Fast and Faster R-CNN  Papers:
 Fully Convolutional Network U-Net &amp;amp; its experiments on medical data! DeepLab Axial DeepLab  </description>
    </item>
    
    <item>
      <title>Novel architectures and tricks to train them</title>
      <link>/archi/</link>
      <pubDate>Mon, 18 Jun 1956 00:00:00 +0000</pubDate>
      
      <guid>/archi/</guid>
      <description>The lecture 
After the lecture   The Illustrated Transormer (NLP)
  Blog on transformers for Vision
  Suuuuuuuper long list of the recent papers for transformers for Vision
  More tricks to train a DNN from Karpathy
  </description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>About</description>
    </item>
    
    <item>
      <title>Deep Neural Network</title>
      <link>/quiz/dnn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/quiz/dnn/</guid>
      <description>Click to reveal answer
           $(document).ready(function () { launchQuiz( &#34;Introduction to Deep Learning&#34;, eval(&#34;[\n {Front: \&#34;What are the two passes done in a neural network?\&#34;, Back: \&#34;Forward and backward passes\&#34;},\n {Front: \&#34;What is the formula of a single neuron?\&#34;, Back: \&#34;\\\\(f(x) = w x + b\\\\)\&#34;},\n {Front: \&#34;What is the shape of \\\\(w\\\\) in a single neuron?\&#34;, Back: \&#34;</description>
    </item>
    
    <item>
      <title>Search</title>
      <link>/search/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/search/</guid>
      <description>search</description>
    </item>
    
  </channel>
</rss>
