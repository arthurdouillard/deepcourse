[{"content":"This lesson has more math than usual, but it\u0026rsquo;s important that we don\u0026rsquo;t shy away the theory behind deep learning. It will help us build a strong understanding of the algorithms we will code in later sections. If you feel overwhelmed by the math here, please see this review first.\nWe are talking of neural networks, then what is a neuron? Certainly not the kind that is in your brain. While biologically inspired, artificial neurons are very different from the real counterpart, thus for now put aside any comparison with brains.\nThe neurons, or parameters, of our network are floating values. A neuron could be 0.7964, -102.329, or 3912.20238. We group those neurons in matrices, often denoted \\(W\\). These matrices are often accompanied by a bias \\(b\\).\nSingle Output Network Let\u0026rsquo;s have an example of very simple network:\n[Image of a Linear regression]\nLet\u0026rsquo;s decompose this network in three parts:\nInput:\n The input is an image of a digit: \\(X \\in \\mathcal{R}^{W \\times H \\times 1}\\). The first two dimensions \\(W\\) \u0026amp; \\(H\\) are respectively the width and height of the image. The last dimension correspond to the number of channels. Because here the image is in grayscale, there is only one channel, but with color images we have three channels: Red, Blue, and Green (RGB). The network we are learning right now only accepts vectors, not tensors. Thus we are flattening the input into a vector with a single dimension of length \\(W \\times H\\).  Neurons / Parameters:\n We have \\(W \\in \\mathcal{R}^{1 \\times W \\times H}\\) and \\(b \\in \\mathcal{R}\\), respectively the weights and bias. The operation \\(W^T X\\) results in a single output value \\(z \\in \\mathcal{R}\\). Thus \\(z\\) can have a value from \\(-\\infty\\) to \\(+\\infty\\).  Activation:\n We use a non-linear activation \\(\\sigma(z) = \\frac{1}{1 + e^{-z}} \\in [0, 1]\\) called sigmoid It allows us to map \\(z\\) to a probability \\(o\\).  This is a single-layer neural network that can classify images into two classes.\nMultiple Outputs Network Multi-layers Networks Activation Functions Backpropagation Initialization Optimizers ","permalink":"/dnn/","summary":"This lesson has more math than usual, but it\u0026rsquo;s important that we don\u0026rsquo;t shy away the theory behind deep learning. It will help us build a strong understanding of the algorithms we will code in later sections. If you feel overwhelmed by the math here, please see this review first.\nWe are talking of neural networks, then what is a neuron? Certainly not the kind that is in your brain. While biologically inspired, artificial neurons are very different from the real counterpart, thus for now put aside any comparison with brains.","title":"Deep Neural Network"},{"content":"            Welcome to this course on Deep Learning for Computer Vision!\nThis course will teach you the fundamentals of Deep Learning applied to images. Each topic will be covered under three different type of materials:\n Lessons, such as this one, accompanied with slides and small quizzes between chapters; Practices, with Google Colab notebooks where we will code actual algorithms; And, quizzes, where you will test your recall on the important notions.  Lessons and practices are essential to learn new materials. However knowledge can be brittle, it\u0026rsquo;s then best to test it with active recall through quizzes. If you\u0026rsquo;re really motivated, you can also download the Anki decks I provide for each lessons. Anki is a space-repetition tools to ingrain deeply knowledge in your memory, and to basically never forget it. I\u0026rsquo;m not kidding, it\u0026rsquo;s a marvel.\nNo more chitchat, let\u0026rsquo;s start.\nA Bit Of History In 1956, the summer workshop of Darmouth was held with prestigious participants (Minsky, Shannon, McCarthy, etc.). This event marked the starting point of Artificial Intelligence as a field on its own. The hopes were high: beating a chess champion, prove math theorems, replace human\u0026rsquo;s works. Following this meeting, Rosenblatt created the perceptron in 1958, a very simplified model of a biological neuron that aimed to classify images. Yes, \u0026ldquo;artificial neural networks\u0026rdquo; and \u0026ldquo;computer vision\u0026rdquo; aren\u0026rsquo;t new.\nDespite these initial progresses, the field suffered from severe cutbacks in funding leading to the first AI winter in the 70s. The expectation were too great compared to what was achieved. This phenomenon occurred again in the 80s. At this point, practical applications came mainly from other approaches than neural networks, the Support-Vector Machines (SVMs) invented in 1992 proved to be useful for the decades to come.\nDuring the same time, Le Cun developped the first Convolutional Neural Network (CNN) which was specialized to work on images. Fast-forward a few years, Deep CNNs became the kind of image classification, first in 2011 with Ciresan, and later in 2012 with AlexNet of Krizhevsky.\nDeep Learning 101 In this course, we will first code a perceptron, the early CNN of Le Cun, and models similar to AlexNet. Then, we will explore the more recent approaches made to Deep Learning. At this point, you may be confused by the terminology: artificial intelligence, machine learning, neural networks, deep learning\u0026hellip; What\u0026rsquo;s their differences?\n    AI vs ML vs DL\n  Machine Learning is a subfield of Artificial Intelligence based on statistical methods. Deep Learning is a subfield of Machine Learning based on deep neural networks.\nDeep Neural Networks, that I\u0026rsquo;ll now abbreviate to DNNs, are made of different modules, exactly like Legos and can be seen as a single function \\(f\\):\n    Computation graph\n  On the left, \\(X\\) is the input. It can be an image, a text, a sound, etc. On the right, \\(f(X)\\) is the result of the function \\(f\\) when applied on \\(X\\). The DNN showcased here is made of four blocks: two linear operations (\\(W_0\\), \\(W_1\\)) and two non-linear operations (\\(\\sigma\\), \\(\\sigma\\)). The former holds parameters which are also known as the network\u0026rsquo;s neurons. The latter are without any parameters.\nWe want to learn the parameters stored in \\(W_0\\) and \\(W_1\\) in order to have a function \\(f\\) suitable to our goal which could be classifying cats and dogs from an image. Following this example, our result \\(f(X)\\) would be a float value between 0 and 1. 0 would indicate that our network is certain that \\(X\\) is an image of cat, and respectively for 1 an image of dog. Most of the time our network is not fully confident and \\(f(X)\\) could be 0.2, 0.4, or even 0.7.\nSo, we want to make sure that \\(f\\) is as certain as possible \u0026mdash;and right\u0026ndash; given an image \\(X\\). To do so, we need a loss function (also called cost function). This new function has to penalize our network if it makes a mistake (i.e. saying cat for a dog image) or if it is too uncertain (i.e. 0.47 for a cat image):\n    Example of the loss function\n  A small loss means that our model is not that bad, and we shouldn\u0026rsquo;t change too much its parameters. On the other hand, a high loss means that we should change a lot of parameters in order to improve.\nIn this course, we will cover how to design our function \\(f\\) (also called model or architecture), what loss function we can choose, and how to actually update our parameters.\nApplications There is a lot of hype currently going about Deep Learning, but is there any actual applications? Yes there are!\n When Siri, Alexa, or Ok Google listen and understand your oral command When your phone unlock itself when it detects your face, like Apple\u0026rsquo;s FaceID Google translate or even your Google search Autonomous driving Faster analysis for radiology AlphaFold 2 recently greatly improved the protein folding problem Better resolution of video games with more FPS with Nvidia\u0026rsquo;s DLSS Faster resolution of computationally intensive physics simulation  And many more.\nEcosystem Deep Learning and Data Science, both in research and industry, is mainly done with the Python programming language. While Python is a very slow language, computation heavy tasks are deferred to a faster language like Fortran or C++.\nFurthermore while programming is usually done on CPUs, it\u0026rsquo;s still too slow for the large amount of compute that deep neural networks need. Thus we will need another piece of hardware: GPUs. Originally designed for computer graphic applications that also do a lot of algebra. This is also the main reason why Deep Learning didn\u0026rsquo;t work before recently, we didn\u0026rsquo;t have the right hardware!\nUnfortunately, GPUs are quite expensive. For this course all coding practice will be done on Google Colab which lend for free GPUs for a few hours. However, you\u0026rsquo;re free to download the exercises and run it on your favorite setting.\nFinally, several Deep Learning frameworks exist: Pytorch, Keras / Tensorflow, Jax, etc. Although Tensorflow greatly improved with its second version, we will use PyTorch. It\u0026rsquo;s a personal opinion, but I feel PyTorch to be more Pythonic and its API is also clearer. But overall, it doesn\u0026rsquo;t make a big difference; if you master PyTorch, we can learn in no time Tensorflow.\nQuiz Now, let\u0026rsquo;s check with a quick quiz our understanding of this chapter:\n   Click to reveal answer\n           $(document).ready(function () { launchQuiz( \"Introduction to Deep Learning\", eval(\"[\\n {Front: \\\"Are artificial neural networks recent?\\\", Back: \\\"No! It dates back from 1958 and Rosenblatt's Perceptron\\\"},\\n {Front: \\\"Is artificial intelligence only Deep Learning?\\\", Back: \\\"No it's only a sub-domain\\\"},\\n {Front: \\\"When learning of Neural Network, which part of it is modified?\\\", Back: \\\"Its parameters\\\"},\\n {Front: \\\"How do we call the function that penalize the Neural Network when it makes mistakes?\\\", Back: \\\"Loss function\\\"},\\n {Front: \\\"What is the main reason why neural networks didn't work in the XXth century?\\\", Back: \\\"Because of a lack of \\u003cb\\u003ecomputational power\\u003c/b\\u003e\\\"},\\n {Front: \\\"What programming language is mainly used in Deep Learning?\\\", Back: \\\"Python\\\"},\\n {Front: \\\"Python is slow, then why Deep Learning mainly exists on Python?\\\", Back: \\\"Because Python is binded to efficient C++\\\"},\\n {Front: \\\"What is the fastest hardware to train a neural network? CPU or GPU?\\\", Back: \\\"GPU\\\"},\\n]\"), false, 1 , false ); });  ","permalink":"/introduction/","summary":"Welcome to this course on Deep Learning for Computer Vision!\nThis course will teach you the fundamentals of Deep Learning applied to images. Each topic will be covered under three different type of materials:\n Lessons, such as this one, accompanied with slides and small quizzes between chapters; Practices, with Google Colab notebooks where we will code actual algorithms; And, quizzes, where you will test your recall on the important notions.","title":"Introduction"},{"content":"Hello! This is the DeepCourse and I\u0026rsquo;m Arthur, a PhD student that loves sharing what I\u0026rsquo;m learning. This course is the new version of a lectures serie I\u0026rsquo;m giving every Fall at the French engineering school EPITA.\nWhat is this Course? This course comprises several methodologies that I think are useful for students:\n First head to the lesson (in yellow) and learn the new topics Each lesson is ended by a small quiz in order to assess that you didn\u0026rsquo;t miss anything important Then practice by coding algorithms in the multiple exercise session (in orange) Do a larger quiz to see again if you understood well every aspect of the lesson by doing active recall  And for the motivated, a fifth step:\nDownload the Anki associated, and be a lifelong-learner with space-repetition!  What is Anki? Whether you\u0026rsquo;re still in school or not, it\u0026rsquo;s very probable that you passed several classes by cramming knowledge. It may have provided you a good grade, but you also probably forgot a lot of it.\nCould we learn to have a good grade AND not forgetting long-term AND not passing hours per day to review the topics acquired years ago?\n Yes, we can.  The magic resides in space-repetition. Gwern.net has made a great overview of this concept, but to put it shortly it is the act of reviewing a fact when you\u0026rsquo;re about to forget it. The following image (source) shows the forgetting curve once we memorized the fact for the first time:\n    Projected forgetting curve and space-repetition\n  Reviewing several times, at longer interval each time \u0026ndash;if you do good, helps to avoid forgetting.\nBecause the intervals are getting longer gradually, you won\u0026rsquo;t feel overwhelmed under many cards. For example, I learned a bit more than 8000 cards in various subjects but I never have more than a hundred cards to review per day, less than 20min.\nAnki is one of the most popular space-repetition software (SRS), mainly because of its vibrant community that shares decks and addons. It is available on Windows, Mac, Linux, Web browsers, android, and iOS. Note that all but the latter are free.\nI cannot stress enough how space-repetition is amazing. Please try it.\nAcknowledgements I had many inspiration for this course, here are some:\n Polytechnique\u0026rsquo;s master of Data Science Stanford\u0026rsquo;s famous CS231n  ","permalink":"/about/","summary":"About","title":"About"},{"content":"","permalink":"/search/","summary":"search","title":"Search"}]