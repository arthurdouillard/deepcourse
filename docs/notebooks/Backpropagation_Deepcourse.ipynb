{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Backpropagation - Deepcourse",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA6tTUFMsOVA"
      },
      "source": [
        "<center><h1>Backpropagation From Scratch</h1></center>\n",
        "\n",
        "<center><h2><a href=\"https://arthurdouillard.com/deepcourse/\">Course link</a></h2></center>\n",
        "\n",
        "To keep your modifications in case you want to come back later to this colab, do *File -> Save a copy in Drive*.\n",
        "\n",
        "If you find a mistake, or know how to improve this notebook, please open an issue [here](https://github.com/arthurdouillard/deepcourse/issues).\n",
        "\n",
        "In this notebook, we'll code a **Logistic Regression** and a two-layers **Multi-Layer Perceptron** (MLP) from scratch.\n",
        "\n",
        "We'll use **pytorch** a deep learning framework. It provides many utilities for neural networks (layers, optimizers, automatic differentiation), but we'll only use its API to manipulate tensors as we would with numpy.\n",
        "\n",
        "The goal will be to learn a model with **backpropagation** in order to classify digits (0, 1, 2, ..., 9) from images.\n",
        "\n",
        "---\n",
        "\n",
        "# 1. Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SssNn_kZqgkA"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow5mTbXbqgkA"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digits = load_digits()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39IslwwyqgkA"
      },
      "source": [
        "X = torch.tensor(digits[\"images\"]).float()\n",
        "Y = torch.tensor(digits[\"target\"]).long()\n",
        "\n",
        "print(f\"Images shape: {X.shape}, targets shape: {Y.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdlG3T1ItnqM"
      },
      "source": [
        "We have 1797 images, each of size 8x8. As you see, there is no **channel dimension** meaning that our images are in grayscale. Which is ok for now, as we only want to classify digits.\n",
        "\n",
        "The targets shape is 1797 because, for each image a target is simply an integer representing the digit.\n",
        "\n",
        "Now that our data is loaded, we need to visualize it. Always look at your data before doing anything! Something may be wrong with the data (not in this case)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "580c7iZbqgkA"
      },
      "source": [
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(X[42], cmap=\"gray\")\n",
        "plt.title(f\"Digit: {Y[42]}\");\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(X[64], cmap=\"gray\")\n",
        "plt.title(f\"Digit: {Y[64]}\");\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(X[1337], cmap=\"gray\")\n",
        "plt.title(f\"Digit: {Y[1337]}\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8mH_DnCubqU"
      },
      "source": [
        "For this dataset, the pixels are coded with 4 bits, meaning that we can have values from 0 to 16 ($2^4$). In deep learning, it's very important to normalize the data in order to have all inputs of relatively the same magnitude.\n",
        "\n",
        "**Beware**: this preprocessing must be done for both the train and test sets! A lot of bugs come from using a slightly different preprocessing between the two sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKyjzJB2qgkA"
      },
      "source": [
        "print(f\"Min and max value of images pixels [{X.min()}, {X.max()}]\")\n",
        "X = X / 16\n",
        "print(f\"Min and max value of normalized images pixels [{X.min()}, {X.max()}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXAFeiDh1_q4"
      },
      "source": [
        "MLP only accepts inputs that are vectors, thus we will flatten our images into vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWV7bkVLqgkA"
      },
      "source": [
        "# Flatten images as vectors\n",
        "print(f\"Images shape: {X.shape}\")\n",
        "X = X.view(X.shape[0], -1)\n",
        "print(f\"Flatten images shape: {X.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GndkrKA82b_Y"
      },
      "source": [
        "---\n",
        "\n",
        "# 2. Activation Functions\n",
        "\n",
        "Now that the data is loaded and preprocessed, we need to code the non-linear activation functions that are essential to deep learning.\n",
        "\n",
        "First let's start by `softmax`, the final activation, that will give us a probability per digit. It takes as input a vector of **logits** (the final outputs of the network before softmax, one value per digit) and returns a vector of probabilities.\n",
        "\n",
        "Here is the formula for the $i^\\text{th}$ probability:\n",
        "\n",
        "$$\\text{softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muyzo7ohqgkA"
      },
      "source": [
        "def softmax(x):\n",
        "  pass # TODO\n",
        "\n",
        "print(softmax(torch.tensor([1., 2., 3.])))\n",
        "print(softmax(torch.tensor([3., -0.12, -4.2, 9])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1oB0x5JAmQk"
      },
      "source": [
        "# Execute this cell to see the solution, but try to do it by yourself before!\n",
        "!wget https://raw.githubusercontent.com/arthurdouillard/deepcourse/master/static/code/backpropagation/softmax_base.py\n",
        "%pycat softmax_base.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOngg3WUqgkA"
      },
      "source": [
        "Because the output vector is a probability distribution, all individual probabilities should sum to 1, let's check that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzSbPCp0qgkA"
      },
      "source": [
        "print(softmax(torch.tensor([1., 2., 3.])).sum())\n",
        "print(softmax(torch.tensor([3., -0.12, -4.2, 9])).sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90nm0_QhqgkA"
      },
      "source": [
        "What if our model is very very confident about class 0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBXSeneIqgkA"
      },
      "source": [
        "softmax(torch.tensor([234., 3., 4.]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJOKxUJYqgkA"
      },
      "source": [
        "Why did we have a NaN? How can we fix it?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iUUOFeXqgkA"
      },
      "source": [
        "def softmax(x):\n",
        "  pass # TODO\n",
        "\n",
        "softmax(torch.tensor([234., 3., 4.]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJdkLBjmA_Z8"
      },
      "source": [
        "# Execute this cell to see the solution, but try to do it by yourself before!\n",
        "!wget https://raw.githubusercontent.com/arthurdouillard/deepcourse/master/static/code/backpropagation/softmax_nan.py\n",
        "%pycat softmax_nan.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg1ZbNDOqgkA"
      },
      "source": [
        "Perfectly identical:\n",
        "\n",
        "$$\\operatorname{softmax}(\\mathbf{x} - c)_i = \\frac{e^{x_i - c}}{\\sum_j e^{x_j - c}}$$\n",
        "$$\\operatorname{softmax}(\\mathbf{x} - c)_i = \\frac{e^{-c} e^{x_i}}{e^{-c} \\sum_j e^{x_j}}$$\n",
        "$$\\operatorname{softmax}(\\mathbf{x} - c)_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
        "$$\\operatorname{softmax}(\\mathbf{x} - c)_i = \\operatorname{softmax}(\\mathbf{x})_i$$\n",
        "\n",
        "\n",
        "In practice, we will have mini-batch, i.e. our probabilities tensor will be of shape $(B, C)$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slZIcZAGqgkA"
      },
      "source": [
        "x = torch.tensor([\n",
        "    [1., 2., 3.],\n",
        "    [4., 9., -12.]\n",
        "])\n",
        "probabilities = softmax(x)\n",
        "\n",
        "print(probabilities.sum())\n",
        "print(probabilities.sum(dim=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyh_IkhSqgkA"
      },
      "source": [
        "The whole batch probabilities sum to 1! That's not what we want. What did go wrong? Remember that most pytorch function can be applied only alongside a dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f__TpQHOqgkA"
      },
      "source": [
        "def softmax(x):\n",
        "  pass # TODO\n",
        "\n",
        "x = torch.tensor([\n",
        "    [1., 2., 3.],\n",
        "    [4., 9., -12.]\n",
        "])\n",
        "probabilities = softmax(x)\n",
        "\n",
        "print(probabilities.sum(dim=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KY_36w9BO9Q"
      },
      "source": [
        "# Execute this cell to see the solution, but try to do it by yourself before!\n",
        "!wget https://raw.githubusercontent.com/arthurdouillard/deepcourse/master/static/code/backpropagation/softmax_batch.py\n",
        "%pycat softmax_batch.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJNcpbJBqgkA"
      },
      "source": [
        "Now, what about a loss? Let's code the cross-entropy!\n",
        "\n",
        "Tips:\n",
        "- Remember about the dimensions, we have mini-batches\n",
        "- Log of 0 is undefined, what trick can we do then?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JmzXQyXqgkA"
      },
      "source": [
        "def cross_entropy(probs, targets):\n",
        "  pass  # TODO\n",
        "\n",
        "probs = torch.tensor([\n",
        "    [0.9, 0.1],\n",
        "    [0.7, 0.3],\n",
        "    [0.2, 0.8],\n",
        "    [0.6, 0.4]\n",
        "])\n",
        "\n",
        "targets = torch.eye(2)[torch.tensor([0, 0, 0, 1])]\n",
        "cross_entropy(probs, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgwDqpm1BaSQ"
      },
      "source": [
        "# Execute this cell to see the solution, but try to do it by yourself before!\n",
        "!wget https://raw.githubusercontent.com/arthurdouillard/deepcourse/master/static/code/backpropagation/ce.py\n",
        "%pycat ce.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MECroglyqgkA"
      },
      "source": [
        "Loss is most important when we are wrong by a large margin. Likewise it is smallest when we are extremely confident. Now you should see that the cross-entropy is maximizing the confidence (also known as the *likelihood*) into the ground-truth class.\n",
        "\n",
        "Try running the cross-entropy with different probabilities to get an intuition about it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s_Sul3sZjks"
      },
      "source": [
        "---\n",
        "\n",
        "# 3. Logistic Regression\n",
        "\n",
        "Now that we have our `softmax` activation and `cross-entropy` loss, we can code a **logistic regression**. Behind this fancy name, it's simply a 1-layer neural network followed a softmax.\n",
        "\n",
        "Here is the forward formula:\n",
        "\n",
        "$$\\tilde{\\mathbf{y}} = \\mathbf{X}\\mathbf{W} + \\mathbf{b}$$\n",
        "$$\\hat{\\mathbf{y}} = \\text{softmax}(\\tilde{\\mathbf{y}})$$\n",
        "$$\\mathcal{L} = -\\frac{1}{B} \\sum_{b=1}^B \\log \\hat{\\mathbf{y}}_y $$\n",
        "\n",
        "With $\\mathbf{X} \\in \\mathbb{R}^{B \\times N}$, $\\mathbf{W} \\in \\mathbb{R}^{N \\times C}$, and $\\mathbf{b} \\in \\mathbb{R}^{C}$. With $B$ being the batch size, $N$ the number of input pixels, and $C$ the number of classes.\n",
        "\n",
        "For the backward, We can simplify formulas with a shortcut by taking directly the gradient of the loss $\\mathcal{L}$ with relation to (w.r.t) the logits $\\tilde{\\mathbf{y}}$ (*see course for details*):\n",
        "\n",
        "$$\\nabla_\\tilde{\\mathbf{y}} \\mathcal{L} = \\hat{\\mathbf{y}} - \\mathbf{y}$$\n",
        "\n",
        "Only two gradients are of interest: the one with relation to (w.r.t) the weights $\\mathbf{W}$ and $\\mathbf{b}$, the neurons we want to update. \n",
        "\n",
        "$$\\nabla_\\mathbf{W} \\mathcal{L} = (\\nabla_\\mathbf{W} \\tilde{\\mathbf{y}})^T \\nabla_\\tilde{\\mathbf{y}} \\mathcal{L}$$\n",
        "$$\\nabla_\\mathbf{b} \\mathcal{L} = \\nabla_\\tilde{\\mathbf{y}} \\mathcal{L}$$\n",
        "\n",
        "**Hint**: Look at the shape of each tensor if you're confused, i.e., the gradient $\\nabla_\\mathbf{W} \\mathcal{L}$ should have the same shape as $\\mathbf{W}$!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJaDibEpqgkA"
      },
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, input_size, nb_classes, learning_rate=0.5):\n",
        "        self.w =  # TODO\n",
        "        self.b =  # TODO\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return  # TODO\n",
        "    \n",
        "    def fit(self, inputs, targets, train=True):\n",
        "        # TODO\n",
        "        return loss\n",
        "    \n",
        "    def backward(self, inputs, probs, targets):\n",
        "        # TODO, should be called by `fit`\n",
        "        \n",
        "    def accuracy(self, inputs, targets):\n",
        "        # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNYK_VyOyWJU"
      },
      "source": [
        "# Execute this cell to see the solution, but try to do it by yourself before!\n",
        "!wget https://raw.githubusercontent.com/arthurdouillard/deepcourse/master/static/code/backpropagation/logreg.py\n",
        "%pycat logreg.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIFAn7dPqgkA"
      },
      "source": [
        "model = LogisticRegression(X.shape[1], len(torch.unique(Y)), 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izky6kr8yz9I"
      },
      "source": [
        "Let's measure the accuracy of the untrained model. What does it, roughtly, correspond to? Any idea why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpMAFwqEqgkA"
      },
      "source": [
        "model.accuracy(X, Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfbVsT_hzBQs"
      },
      "source": [
        "Let's train! We are going to see the whole dataset `nb_epochs` times, by chunk of `batch_size` images.\n",
        "\n",
        "**Note** that we are training and testing on the same set here for simplicity, but in later courses, or in real-life, don't do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGz4Vkj0qgkA"
      },
      "source": [
        "batch_size = 32\n",
        "nb_epochs = 10\n",
        "\n",
        "epochs, accuracies, losses = [], [], []\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    for batch_index in range(0, len(X), batch_size):\n",
        "        batch_X = X[batch_index:batch_index + batch_size]\n",
        "        batch_Y = Y[batch_index:batch_index + batch_size]\n",
        "    \n",
        "        loss = model.fit(batch_X, batch_Y)\n",
        "        \n",
        "    loss = model.fit(X, Y, train=False)\n",
        "    acc = model.accuracy(X, Y)\n",
        "    \n",
        "    print(f\"Epoch: {epoch}, loss: {loss}, accuracy: {acc}\")\n",
        "    epochs.append(epoch)\n",
        "    losses.append(loss)\n",
        "    accuracies.append(acc)\n",
        "    \n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, accuracies)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"accuracy\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQhyi4XgqgkA"
      },
      "source": [
        "--- \n",
        "\n",
        "# 4. Multi-Layer Perceptron\n",
        "\n",
        "Now, let's build a Multi-Layer Perceptron, aka a neural network with hidden layers.\n",
        "\n",
        "Hidden layers imply hidden activations. `tanh` is already implemented for you: `torch.tanh`.\n",
        "\n",
        "This function is applied **element-wise**, meaning that it is applied independently on every point of the tensor (not like softmax). We now need the gradient of this function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKInLQbfqgkA"
      },
      "source": [
        "def grad_tanh(tanh_results):\n",
        "    return  # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlsK162Rz8fU"
      },
      "source": [
        "# Execute this cell to see the solution, but try to do it by yourself before!\n",
        "!wget https://raw.githubusercontent.com/arthurdouillard/deepcourse/master/static/code/backpropagation/gradtanh.py\n",
        "%pycat gradtanh.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrmEvuvd0HTg"
      },
      "source": [
        "Now the MLP is like the Logistic Regression we coded previously, but with hidden layers!\n",
        "\n",
        "We'll only start with one hidden layer to start. The forward should be straightforward, and for the backward try to derive it by yourself. If you're really stuck, you can have a look at the code solution, or the course. But after you've tried enough!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKgrbNKpqgkA"
      },
      "source": [
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size, nb_classes, learning_rate=0.5):\n",
        "        self.w_hidden =  # TODO\n",
        "        self.b_hidden =  # TODO\n",
        "        \n",
        "        self.w_output =  # TODO\n",
        "        self.b_output =  # TODO\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "    def forward(self, x):\n",
        "       # TODO\n",
        "       # Remember to keep all intermediary values that are needed for the backward pass\n",
        "        \n",
        "    def fit(self, inputs, targets, train=True):\n",
        "        # TODO\n",
        "        return loss\n",
        "    \n",
        "    def backward(self, inputs, targets, h_tilde, h, logits, probs):\n",
        "        # TODO\n",
        "        \n",
        "    def accuracy(self, inputs, targets):\n",
        "        # TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSEVkT3P1NRo"
      },
      "source": [
        "The same code used for the Logisitic Regression can also be used for the MLP.\n",
        "\n",
        "That's the beauty of it, as long as our model can take in inputs images and predicts their digits, we don't care about the internals in the training loops:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMXZB__6qgkA"
      },
      "source": [
        "model = MLP(X.shape[1], 50, len(torch.unique(Y)), 0.5)\n",
        "model.accuracy(X, Y)\n",
        "\n",
        "batch_size = 32\n",
        "nb_epochs = 10\n",
        "\n",
        "epochs, accuracies, losses = [], [], []\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    for batch_index in range(0, len(X), batch_size):\n",
        "        batch_X = X[batch_index:batch_index + batch_size]\n",
        "        batch_Y = Y[batch_index:batch_index + batch_size]\n",
        "    \n",
        "        model.fit(batch_X, batch_Y)\n",
        "        \n",
        "    loss = model.fit(X, Y, train=False)\n",
        "    acc = model.accuracy(X, Y)\n",
        "    \n",
        "    print(f\"Epoch: {epoch}, loss: {loss}, accuracy: {acc}\")\n",
        "    epochs.append(epoch)\n",
        "    losses.append(loss)\n",
        "    accuracies.append(acc)\n",
        "    \n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, accuracies)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"accuracy\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzGlhtIV1ml8"
      },
      "source": [
        "--- \n",
        "\n",
        "# Notebook Summary\n",
        "\n",
        "We learn how to:\n",
        "- code a robust softmax\n",
        "- logistic regression with forward and backward pass\n",
        "- MLP with forward and backward pass\n",
        "\n",
        "# Further Works\n",
        "\n",
        "- Try to implement a MLP with two hidden layers, or three, or as much as the user want, through a simple API.\n",
        "- Does the initialization of the weights matter? Try tweaking it (more on it in later courses)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSjFRrCO2O7M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}