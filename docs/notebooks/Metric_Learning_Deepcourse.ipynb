{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Metric Learning - Deepcourse",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9VQbcDyjPDA"
      },
      "source": [
        "<center><h1>Metric Learning</h1></center>\n",
        "\n",
        "<center><h2><a href=\"https://arthurdouillard.com/deepcourse/\">Course link</a></h2></center>\n",
        "\n",
        "To keep your modifications in case you want to come back later to this colab, do *File -> Save a copy in Drive*.\n",
        "\n",
        "If you find a mistake, or know how to improve this notebook, please open an issue [here](https://github.com/arthurdouillard/deepcourse/issues)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ptcrfb-jBJ8"
      },
      "source": [
        "!rm -rf *.tgz*\n",
        "!wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz\n",
        "!tar zxf lfw-deepfunneled.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhQkPaSOkUFw"
      },
      "source": [
        "%pylab inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFSkRmtTjyco"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "import collections\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMR-vgPdQGJk"
      },
      "source": [
        "We are going to code a Siamese network to do face recognition. We'll use the LFW dataset, where every folder contain one or multiple images of a celebrity:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je8zGGbZkW2U"
      },
      "source": [
        "os.listdir(\"lfw-deepfunneled\")[:10], os.listdir(\"lfw-deepfunneled\")[-10:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtR6PV8sQRvw"
      },
      "source": [
        "We first build two subsets, one for train and one for val. Because we are doing metric learning, we don't need to have the same classes between the two sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIM0ag2jkaHs"
      },
      "source": [
        "names = sorted(os.listdir(\"lfw-deepfunneled\"))\n",
        "name_to_classid = {d: i for i, d in enumerate(names)}\n",
        "classid_to_name = {v: k for k, v in name_to_classid.items()}\n",
        "\n",
        "def build_subset(names, start_index, end_index):\n",
        "  classid_to_paths = collections.defaultdict(list)\n",
        "  c = 0\n",
        "  for name in names[start_index:end_index]:\n",
        "    class_id = name_to_classid[name]\n",
        "    for image_name in os.listdir(f\"lfw-deepfunneled/{name}\"):\n",
        "      classid_to_paths[class_id].append(f\"lfw-deepfunneled/{name}/{image_name}\")\n",
        "      c += 1\n",
        "\n",
        "  print(f\"Number of person: {len(classid_to_paths)}\")\n",
        "  print(f\"Number of images: {c}\")\n",
        "\n",
        "  return classid_to_paths\n",
        "\n",
        "\n",
        "print(\"Build train...\")\n",
        "train_set = build_subset(names, 0, 500)\n",
        "print(\"Build test...\")\n",
        "val_set = build_subset(names, 500, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6lBOd6dQe4Y"
      },
      "source": [
        "Notice that most celebrities have only 1 image! We definitely cannot use them to do positive pairs, but we can still keep them for negative pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6WAGShoQdfU"
      },
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "train_occur = {classid_to_name[class_id]: len(paths) for class_id, paths in train_set.items()}\n",
        "plt.hist(train_occur.values())\n",
        "plt.title(\"Distribution of train set\")\n",
        "plt.xlabel(\"Nb images / person\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "val_occur = {classid_to_name[class_id]: len(paths) for class_id, paths in val_set.items()}\n",
        "plt.hist(val_occur.values())\n",
        "plt.title(\"Distribution of val set\")\n",
        "plt.xlabel(\"Nb images / person\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW-bP7IuQo_Q"
      },
      "source": [
        "Here are the most represented celebrities of our train set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYQlL_xmk67s"
      },
      "source": [
        "for name, nb in sorted(train_occur.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
        "  print(f\"{name}: {nb} images\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhLDt4lmQuCs"
      },
      "source": [
        "We now build pairs, we will balance our dataset to have as much negative as positive pairs.\n",
        "\n",
        "Note that we could compute the pair on the fly at each batch, but it helps the model convergence to see multiple times the same pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUxTBl9TozfQ"
      },
      "source": [
        "def build_pairs(classid_to_paths, pair_per_class=50):\n",
        "  pairs = []\n",
        "  classes = set(classid_to_paths.keys())\n",
        "\n",
        "  for class_id in classid_to_paths:\n",
        "    nb = len(classid_to_paths[class_id])\n",
        "    if nb == 1:\n",
        "      continue\n",
        "\n",
        "    for _ in range(min(pair_per_class, nb)):\n",
        "      # pos\n",
        "      pairs.append((\n",
        "          random.choice(classid_to_paths[class_id]),\n",
        "          random.choice(classid_to_paths[class_id]),\n",
        "          1.0\n",
        "      ))\n",
        "\n",
        "      # neg\n",
        "      neg_classes = classes - {class_id}\n",
        "      neg_class_id = random.choice(list(neg_classes))\n",
        "\n",
        "      pairs.append((\n",
        "          random.choice(classid_to_paths[class_id]),\n",
        "          random.choice(classid_to_paths[neg_class_id]),\n",
        "          0.0\n",
        "      ))\n",
        "\n",
        "  return pairs\n",
        "\n",
        "\n",
        "class SiameseDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, classid_to_paths, transform, pair_per_class=20):\n",
        "    self.transform = transform\n",
        "    self.pairs = build_pairs(classid_to_paths, pair_per_class=pair_per_class)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.pairs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    p1, p2, y = self.pairs[index]\n",
        "\n",
        "    left_img = Image.open(p1).convert('RGB')\n",
        "    right_img = Image.open(p2).convert('RGB')\n",
        "\n",
        "    left_img = self.transform(left_img)\n",
        "    right_img = self.transform(right_img)\n",
        "\n",
        "    return left_img, right_img, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0miCHl3YhW8"
      },
      "source": [
        "Let's build the data augmentations and the loaders. \n",
        "\n",
        "You can change the train augmentations and see how performance change. But beware, too much augmentation is often detrimental to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctwwaXOLRpXA"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((100, 100)),\n",
        "    transforms.CenterCrop((80, 80)),\n",
        "    transforms.ColorJitter(brightness=0.2),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((100, 100)),\n",
        "    transforms.CenterCrop((80, 80)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = SiameseDataset(train_set, train_transform, pair_per_class=50)\n",
        "val_dataset = SiameseDataset(val_set, val_transform, pair_per_class=50)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "len(train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd3MsiOhYvl4"
      },
      "source": [
        "Always, check that the loader output seems correct:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNBGlfFkRsfA"
      },
      "source": [
        "x1, x2, y = next(iter(train_loader))\n",
        "\n",
        "x1.shape, x2.shape, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VcAV78SZBDE"
      },
      "source": [
        "plt.figure(figsize=(20, 6))\n",
        "\n",
        "for i in range(5):\n",
        "  ax = plt.subplot(2, 5, i + 1)\n",
        "  ax.axis('off')\n",
        "  img = (x1[i].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
        "  plt.imshow(img)\n",
        "\n",
        "for i in range(5):\n",
        "  ax = plt.subplot(2, 5, i + 1 + 5)\n",
        "  ax.axis('off')\n",
        "  img = (x2[i].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
        "  plt.imshow(img)\n",
        "  plt.title(\"Same person\" if y[i] == 1.0 else \"Different person\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeR1m-rja1Ts"
      },
      "source": [
        "Now, build by yourself a backbone. You're free to do it as you want, and you can use the ConvBlock I provide for simplicity.\n",
        "\n",
        "\n",
        "Ideally, your number of channels should range from 16 to 128, and you should interleave Max/Avg pooling between conv blocks. You can finish the network by a few fully connected layers, and finish with an embedding size of 50."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJMC1ua1RuK0"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, padding),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "  \n",
        "\n",
        "class Backbone(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # TODO\n",
        "\n",
        "  def forward(self, x):\n",
        "    # TODO\n",
        "    return x\n",
        "\n",
        "\n",
        "# Always check the output shape!\n",
        "Backbone()(torch.randn(32, 3, 80, 80)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UexTwUzgeUWs"
      },
      "source": [
        "# Execute this cell to see the solution, but try to do it by yourself before!\n",
        "!wget https://raw.githubusercontent.com/arthurdouillard/deepcourse/master/static/code/siamese/backbone.py\n",
        "%pycat backbone.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCwdFYOmeb-M"
      },
      "source": [
        "Now, we want a function that will do a pairwise cosine similarity.\n",
        "\n",
        "Meaning given two tensors of shape (n, d), we compute over each row (1, d) the cosine similarity between the two parts.\n",
        "\n",
        "Remember the cosine similarity formula is: \n",
        "\n",
        "$$\\operatorname{cos}(x, y) = \\frac{x \\cdot y}{\\Vert x \\Vert_2 \\Vert y \\Vert_2}$$\n",
        "\n",
        "The function `F.cosine_similarity` in PyTorch already does that, but try to recode it yourself! You can use it to validate your function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy70pbfZZ-ao"
      },
      "source": [
        "def tensor_cosine(x, y):\n",
        "  # Build a pair-wise cosine similarity without using F.cosine_similarity\n",
        "  # Hint, you may want to normalize vectors with F.normalize\n",
        "\n",
        "  # TODO\n",
        "  pass\n",
        "  \n",
        "\n",
        "x = torch.randn(3, 5)\n",
        "y = torch.randn(3, 5)\n",
        "\n",
        "F.cosine_similarity(x, y), tensor_cosine(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_HezbMefke0"
      },
      "source": [
        "# Execute this cell to see the solution, but try to do it by yourself before!\n",
        "!wget https://raw.githubusercontent.com/arthurdouillard/deepcourse/master/static/code/siamese/cos.py\n",
        "%pycat cos.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2VtyIICfn5k"
      },
      "source": [
        "Now let's build our Siamese network! It combines the backbone and the tensor cosine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlutSVV7WAb8"
      },
      "source": [
        "class Siamese(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # TODO\n",
        "\n",
        "  def forward(self, x1, x2):\n",
        "    # TODO\n",
        "    return cos\n",
        "\n",
        "\n",
        "# Check output shape\n",
        "Siamese()(torch.randn(5, 3, 80, 80), torch.randn(5, 3, 80, 80)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M7hWnBHf5k0"
      },
      "source": [
        "# Execute this cell to see the solution, but try to do it by yourself before!\n",
        "!wget https://raw.githubusercontent.com/arthurdouillard/deepcourse/master/static/code/siamese/siamese.py\n",
        "%pycat siamese.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aSII7q8ggaw"
      },
      "source": [
        "We need a **contrastive loss**. Here is the formula:\n",
        "\n",
        "$$y (1 - \\hat{y})^2 + (1 - y) \\operatorname{max}^2(\\hat{y} - m, 0)  $$\n",
        "\n",
        "Notice that the ground-truth ($y = 1$ same person, $y = 0$ different person) acts as a gate between the two parts of the loss.\n",
        "\n",
        "Now code it yourself!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpKRZA76aPHY"
      },
      "source": [
        "def contrastive_loss(pred_simi, gt_simi, margin=0.20):\n",
        "  # TODO\n",
        "  return 0."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy8Fjb1UxEYQ"
      },
      "source": [
        "# Execute this cell to see the solution, but try to do it by yourself before!\n",
        "!wget https://raw.githubusercontent.com/arthurdouillard/deepcourse/master/static/code/siamese/cons.py\n",
        "%pycat cons.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9M_r_wEa29Y"
      },
      "source": [
        "def eval_model(net, loader):\n",
        "  net.eval()\n",
        "  acc, loss = 0., 0.\n",
        "  c = 0\n",
        "  for x1, x2, gt_simi in loader:\n",
        "    with torch.no_grad():\n",
        "      pred_simi = net(x1.cuda(), x2.cuda()).cpu()\n",
        "\n",
        "    loss += contrastive_loss(pred_simi, gt_simi).item()\n",
        "    acc += ((pred_simi > 0.5).float() == gt_simi).sum().item()\n",
        "    c += len(x1)\n",
        "\n",
        "  acc /= c\n",
        "  loss /= len(loader)\n",
        "  net.train()\n",
        "  return round(100 * acc, 2), round(loss, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8KZrITmxRRQ"
      },
      "source": [
        "Let's train. We define the accuracy according to an arbitratry threshold of 0.5.\n",
        "\n",
        "Notice that the margin is set to 0 at first, you can try to change it to get better results. You may try to make it change gradually during training to induce some kind of *curriculum learning* also."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMZ1NBstbs48"
      },
      "source": [
        "net = Siamese().cuda()\n",
        "best_model, best_acc = None, 0.0\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.0005)\n",
        "\n",
        "val_acc, val_loss = eval_model(net, val_loader)\n",
        "print(f\"Random model --> Val loss: {val_loss}, val accuracy: {val_acc}\")\n",
        "epochs = 15\n",
        "margin = 0.0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  losses = 0\n",
        "\n",
        "  for x1, x2, y in train_loader:\n",
        "    x1, x2, y = x1.cuda(), x2.cuda(), y.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    pred_simi = net(x1, x2)\n",
        "    loss = contrastive_loss(pred_simi, y, margin=margin)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch}: Train loss: {round(losses / len(train_loader), 5)}\")\n",
        "\n",
        "  val_acc, val_loss = eval_model(net, val_loader)\n",
        "  print(f\"\\tVal loss: {val_loss}, val accuracy: {val_acc}\")\n",
        "\n",
        "  if best_acc <= val_acc:\n",
        "    print(\"\\tCheckpointing!\")\n",
        "    best_acc = val_acc\n",
        "    best_model = copy.deepcopy(net)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P5tO8Ywxkmw"
      },
      "source": [
        "Now we want to visualize the recall of our model, let's compute all embeddings (here of the val set, but feel free to also try on the train set where results should be better)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLT5mHJbhbMs"
      },
      "source": [
        "names = []\n",
        "embeddings = []\n",
        "paths = []\n",
        "\n",
        "for class_id, class_paths in val_set.items():\n",
        "  for path in class_paths:\n",
        "    paths.append(path)\n",
        "    names.append(classid_to_name[class_id])\n",
        "\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    img = val_transform(img)\n",
        "    with torch.no_grad():\n",
        "      emb = best_model.backbone(img[None].cuda())[0]\n",
        "\n",
        "    embeddings.append(emb)\n",
        "\n",
        "embeddings = torch.stack(embeddings)\n",
        "embeddings.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfP72JTrB9m8"
      },
      "source": [
        "for name, nb in sorted(val_occur.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "  print(f\"{name} ({name_to_classid[name]}): {nb} images\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIICymaHl-dU"
      },
      "source": [
        "selected_name = \"Bill_Clinton\"\n",
        "\n",
        "print(f\"There are {names.count(selected_name)} images with {selected_name}'s face.\")\n",
        "idx = random.choice([i for i, name in enumerate(names) if name == selected_name])\n",
        "\n",
        "similarities = tensor_cosine(embeddings[idx][None], embeddings)\n",
        "closest_indexes = similarities.argsort()[-10:].flip(0)\n",
        "closest_simis = similarities[closest_indexes]\n",
        "print(closest_indexes)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20, 6))\n",
        "\n",
        "for i, (closest_index, closest_simi) in enumerate(zip(closest_indexes, closest_simis), start=1):\n",
        "  ax = plt.subplot(2, 5, i)\n",
        "  ax.axis('off')\n",
        "\n",
        "  path, name = paths[closest_index], names[closest_index]\n",
        "\n",
        "  img = Image.open(path).convert('RGB')\n",
        "  plt.imshow(img)\n",
        "  plt.title(f\"{name}, simi: {round(closest_simi.item(), 2)}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUohkS9Fxt6E"
      },
      "source": [
        "The most similar image (with similarity score of $1.0$) is obviously the image itself. \n",
        "\n",
        "Remark that our model isn't very good, and it happens that the correct person is not even in the top-k...\n",
        "\n",
        "You can try to improve it by:\n",
        "- using more data\n",
        "- larger image size (slower!)\n",
        "- bigger network\n",
        "- tuning of the margin, optimizer, etc.\n",
        "- more augmentation\n",
        "- select hard negative\n",
        "- use a triplet network\n",
        "\n",
        "Notice that the train loss quickly overfit. Would that happen with hard negative mining?"
      ]
    }
  ]
}