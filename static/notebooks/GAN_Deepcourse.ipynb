{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN - Deepcourse",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpCaeRDyqTso"
      },
      "source": [
        "<center><h1>GAN</h1></center>\n",
        "\n",
        "<center><h2><a href=\"https://arthurdouillard.com/deepcourse/\">Course link</a></h2></center>\n",
        "\n",
        "To keep your modifications in case you want to come back later to this colab, do *File -> Save a copy in Drive*.\n",
        "\n",
        "If you find a mistake, or know how to improve this notebook, please open an issue [here](https://github.com/arthurdouillard/deepcourse/issues)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjZk8JqYHeX0"
      },
      "source": [
        "We are going to implement a mix of several GAN architecture that work quite well.\n",
        "\n",
        "It'll be a [DC-GAN](https://arxiv.org/abs/1511.06434) (aka with convolutions), but also a [cGAN](https://arxiv.org/abs/1411.1784) (conditioned on the label of the image to generate), and a [SN-GAN](https://arxiv.org/abs/1802.05957) (with spectral normalization to ease training)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfXRE1lRqLcg"
      },
      "source": [
        "%pylab inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhFz39_Wqajs"
      },
      "source": [
        "import pickle\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.parametrizations import spectral_norm\n",
        "from torch.nn.utils import spectral_norm\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "import torchvision.utils as vutils\n",
        "import torchvision\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIaBQttDnyYQ"
      },
      "source": [
        "Let's load the MNIST dataset. We are going to normalize its pixels to fit in $[-1, +1]$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJKTzp4MtPBs"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "  transforms.Resize(32),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.MNIST('.', train=True, download=True, transform=transform)\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHw5ZxNioDZU"
      },
      "source": [
        "... and visualize!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZlxjUPZdGX0"
      },
      "source": [
        "real_batch = next(iter(loader))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Images\")\n",
        "plt.imshow(vutils.make_grid(real_batch[0][:64], normalize=True).permute(1,2,0))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6UbC9F1CBhQ"
      },
      "source": [
        "This time, we are going to need a one-hot vector instead of the scalar label. Here is a useful function to get that.\n",
        "\n",
        "Note the shape of `(10, 10, 1, 1)`:\n",
        "- The first dimension indicates that we have 10 samples\n",
        "- The second dimension indicates that we have 10 different classes\n",
        "- The last two dimensions are added so that our one-hot vector has similar shape as an image `(height, width)`. This is going to be useful later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPvvzFviivCo"
      },
      "source": [
        "def one_hot(y):\n",
        "  y_onehot = torch.zeros(len(y), 10).to(y.device)\n",
        "  y_onehot.scatter_(1, y[..., None], 1)\n",
        "  return y_onehot[..., None, None]\n",
        "\n",
        "y = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).long()\n",
        "one_hot_y = one_hot(y)\n",
        "\n",
        "one_hot_y.squeeze(), one_hot_y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5LqOgrBeDI4"
      },
      "source": [
        "Now let's define some hyperparameters.\n",
        "\n",
        "Adam is a very robust optimizer that can work in many different settings with a wide range of architectures. However, GANs are particularly difficult to optimize and finding the right hyperparameters for Adam can be akin to black magic. Don't hesitate to try to find better ones!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1hLHoDxIH-I"
      },
      "source": [
        "noise_dim = 100\n",
        "d = 32\n",
        "\n",
        "lr_d = 0.0002\n",
        "lr_g = 0.0002\n",
        "beta_d = (0., 0.9)\n",
        "beta_g = (0., 0.9)\n",
        "epsilon_d = 1e-08\n",
        "epsilon_g = 1e-08\n",
        "\n",
        "# How many updates of the discriminator to do\n",
        "# before doing one update of the generator\n",
        "update_gen_every = 1\n",
        "\n",
        "# We define lambda functions to create\n",
        "# the labels for real and fake images\n",
        "# given a batch size `bs`\n",
        "REAL = lambda bs: torch.ones(bs)\n",
        "FAKE = lambda bs: torch.zeros(bs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-WDwMTjcSvM"
      },
      "source": [
        "Many problems arise with GAN including **mode collapse** where the generator produces the same kind of images or **mode dropping** where the generator fails to produce some details.\n",
        "\n",
        "The key is that the \"battle\" between the generator and discriminator isn't too balanced towards one of the models. Otherwise we can see vanishing or exploding gradients for the discriminator, leading to a very bad GAN...\n",
        "\n",
        "Multiple works have tried to stabilize the training, mainly by enforcing that the discriminator is $K$-Lipschitz (~that its loss is smooth). The most famous paper about that is probably the [Wasserstein GAN](https://arxiv.org/abs/1701.07875). However we are going to implement the [Spectral Normalization (SN) GAN](https://arxiv.org/abs/1802.05957) which is simpler and more efficient.\n",
        "\n",
        "\n",
        "Spectral Normalization is very simple, the weights are divided by its maximum singular value. Thus the weight is 1-Lipschitz:\n",
        "\n",
        "$$\\mathbf{W}_{S N}=\\frac{\\mathbf{W}}{\\sigma(\\mathbf{W})}, \\sigma(\\mathbf{W})=\\max _{\\mathbf{h}: \\mathbf{h} \\neq 0} \\frac{\\|\\mathbf{W h}\\|_{2}}{\\|\\mathbf{h}\\|_{2}}$$\n",
        "\n",
        "We simply need to wrap the discriminator's Conv layers (and FC layers if we had some) with PyTorch's `spectral_norm`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzl-Q4tnHR-E"
      },
      "source": [
        "class SpectralConv(nn.Module):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__()\n",
        "    self.conv = spectral_norm(nn.Conv2d(*args, **kwargs))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "\n",
        "# When defining the discriminator AND generator\n",
        "# instead of using BatchNorm use the variable `norm_layer`\n",
        "# so you can try easily BatchNorm vs InstanceNorm\n",
        "# More on it here:\n",
        "# * https://arthurdouillard.com/post/normalization/\n",
        "\n",
        "# For the discriminator replace all Conv by `conv_disc_layer`\n",
        "# so you can try easily between a classic Conv and a Spectral Conv.\n",
        "\n",
        "norm_layer = nn.InstanceNorm2d  # nn.BatchNorm2d\n",
        "conv_disc_layer = SpectralConv # nn.Conv2d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTJZ-f5nfrN0"
      },
      "source": [
        "First, define the discriminator, which will takes in input both real and fake images and must decide which one is real. Note that the number of channels will be a multiple of $d$ which was defaulted in a previous cell to $32$. \n",
        "\n",
        "For the discriminator only, we also replace all ReLUs by Leaky ReLUs to avoid dying neurons:\n",
        "\n",
        "$$\\text { LeakyRELU }(x)=\\left\\{\\begin{array}{ll}\n",
        "x, & \\text { if } x \\geq 0 \\\\\n",
        "\\text { negative_slope } \\times x, & \\text { otherwise }\n",
        "\\end{array}\\right.$$\n",
        "\n",
        "Define three branches:\n",
        "\n",
        "**Image branch**: takes (real or fake) image in input\n",
        "- one `conv_disc_layer` that produces $2d$ channels, $4\\times 4$ kernel, stride $2$, and padding $1$\n",
        "- One LeakyReLU with a slope of $0.2$.\n",
        "\n",
        "**Label branch**: takes the one-hot label in input\n",
        "- one `conv_disc_layer` that produces $2d$ channels, $4\\times 4$ kernel, stride $2$, and padding $1$\n",
        "- One LeakyReLU with a slope of $0.2$.\n",
        "\n",
        "The output of those two branches will be concatenated and given to the **classifier branch** that produces the binary classification real-vs-fake:\n",
        "- one `conv_disc_layer` that produces $8d$ channels, $4\\times 4$ kernel, stride $2$, and padding $1$\n",
        "- one `norm_layer`\n",
        "- One LeakyReLU with a slope of $0.2$\n",
        "- one `conv_disc_layer` that produces $16d$ channels, $4\\times 4$ kernel, stride $2$, and padding $1$\n",
        "- one `norm_layer`\n",
        "- One LeakyReLU with a slope of $0.2$\n",
        "- one `conv_disc_layer` that produces $1$ channels, $4\\times 4$ kernel, stride $2$, and padding $1$\n",
        "- one sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1HBXS1_fkxU"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.img_branch = nn.Sequential(\n",
        "            # TODO\n",
        "        )\n",
        "        self.label_branch = nn.Sequential(\n",
        "            # TODO\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            # TODO\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Expand label to have the size of the image\n",
        "        y = y.expand(y.shape[0], y.shape[1], x.shape[2], x.shape[3])\n",
        "\n",
        "        x = self.img_branch(x)\n",
        "        y = self.label_branch(y)\n",
        "\n",
        "        xy_concatenated = # TODO\n",
        "\n",
        "        o = self.classifier(xy_concatenated)\n",
        "\n",
        "        return o.squeeze() # go from (bs, 1, 1, 1) to (bs,)\n",
        "\n",
        "\n",
        "Discriminator()(torch.randn(10, 1, 32, 32), one_hot(y)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXmRJ7WNnUD0"
      },
      "source": [
        "# Execute this cell to see the solution, but try to do it by yourself before!\n",
        "!wget https://raw.githubusercontent.com/arthurdouillard/deepcourse/master/static/code/gan/disc.py\n",
        "%pycat disc.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V35-ZHrQkxtM"
      },
      "source": [
        "Now let's build the generator. Again we use the `norm_layer` variable for our normalization layer, however we aren't going to use the `conv_disc_layer` but instead of `nn.ConvTranspose2d` to \"upsample\" the image:\n",
        "\n",
        "```\n",
        "nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0\n",
        "```\n",
        "\n",
        "Again, three branches:\n",
        "\n",
        "**Noise branch**: that takes in input the noise vector:\n",
        "- A ConvTranspose with $8d$ channels, a $4\\times 4$ kernel, stride $1$, padding $0$\n",
        "- a `norm_layer`\n",
        "- a ReLU\n",
        "\n",
        "**Label branch**: that takes the label in input:\n",
        "- A ConvTranspose with $8d$ channels, a $4\\times 4$ kernel, stride $1$, padding $0$\n",
        "- a `norm_layer`\n",
        "- a ReLU\n",
        "\n",
        "The output of those two branches will be concatenated and given to the **generator branch** that produces the fake image:\n",
        "- A ConvTranspose with $8d$ channels, a $4\\times 4$ kernel, stride $2$, padding $0$\n",
        "- a `norm_layer`\n",
        "- a ReLU\n",
        "- A ConvTranspose with $4d$ channels, a $4\\times 4$ kernel, stride $2$, padding $0$\n",
        "- a `norm_layer`\n",
        "- a ReLU\n",
        "- A ConvTranspose with $1$ channels, a $4\\times 4$ kernel, stride $2$, padding $0$\n",
        "- a Tanh\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljQe81sFhJ94"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.noise_branch = nn.Sequential(\n",
        "            # TODO\n",
        "        )\n",
        "        self.label_branch = nn.Sequential(\n",
        "            # TODO\n",
        "        )\n",
        "        self.generator = nn.Sequential(\n",
        "            # TODO\n",
        "        )\n",
        "\n",
        "    def forward(self, z, y):\n",
        "        z = self.noise_branch(z)\n",
        "        y = self.label_branch(y)\n",
        "\n",
        "        zy_concatenated = # TODO\n",
        "        o = self.generator(zy_concatenated)\n",
        "\n",
        "        return o\n",
        "\n",
        "Generator()(torch.randn(10, noise_dim, 1, 1), one_hot(y)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ9r5aOonX8Q"
      },
      "source": [
        "# Execute this cell to see the solution, but try to do it by yourself before!\n",
        "!wget https://raw.githubusercontent.com/arthurdouillard/deepcourse/master/static/code/gan/gen.py\n",
        "%pycat gen.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZDOwT20h50I"
      },
      "source": [
        "Check that the output of the generator has the same dimension as a real image! If not it may lead to unforeseen problems!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg1_IqCajFxE"
      },
      "source": [
        "real_batch[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VcCNXYDjIFg"
      },
      "source": [
        "Now let's code our noise generator and criterion:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFpLheIsQTRE"
      },
      "source": [
        "def sample_z(batch_size):\n",
        "  return torch.randn(batch_size, noise_dim, 1, 1).cuda()\n",
        "\n",
        "criterion = nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFkoNVKijQdc"
      },
      "source": [
        "For simplicity, we code a forward pass in a separate function that we call `train_on_batch`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_lDmtBMiCEQ"
      },
      "source": [
        "def train_on_batch(x_real, y_real, G, D, opt_g, opt_d, n_iter, update_gen_every=5):\n",
        "    opt_g.zero_grad()\n",
        "    opt_d.zero_grad()\n",
        "    \n",
        "    real = REAL(len(x_real)).cuda()\n",
        "    fake = FAKE(len(x_real)).cuda()\n",
        "\n",
        "    # --------------\n",
        "    # First, train G\n",
        "    # --------------\n",
        "    z = sample_z(x_real.size(0))\n",
        "    x_fake = G(z, y_real)\n",
        "    g_loss = criterion(D(x_fake, y_real), real)\n",
        "\n",
        "    if n_iter % update_gen_every == 0:\n",
        "      # This backpropagates from the output of D, all the\n",
        "      # way back into G.\n",
        "      g_loss.backward()\n",
        "      # G's gradient buffers are filled, we can perform\n",
        "      # an optimisation step.\n",
        "      opt_g.step()\n",
        "    x_fake = x_fake.detach()\n",
        "      \n",
        "    # ------------\n",
        "    # Now, train D\n",
        "    # ------------\n",
        "    \n",
        "    # IMPORTANT: D's grad buffers are filled because\n",
        "    # of what we did above.\n",
        "    opt_d.zero_grad()\n",
        "    \n",
        "    d_loss = criterion(D(x_fake, y_real), fake) + \\\n",
        "             criterion(D(x_real, y_real), real)\n",
        "    d_loss.backward()\n",
        "    opt_d.step()\n",
        "    \n",
        "    return g_loss.detach(), d_loss.detach()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV2KtJi7ncs8"
      },
      "source": [
        "We fix a noise vector that we will monitor to see how well our model fares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_6XiOmzTfRU"
      },
      "source": [
        "fixed_noise = sample_z(20).cuda()\n",
        "fixed_y = one_hot(torch.cat((\n",
        "    torch.arange(10), torch.arange(10)\n",
        "))).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZl46CugnhwY"
      },
      "source": [
        "Let's train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olHhZqjOnYx8"
      },
      "source": [
        "epochs = 5\n",
        "n_iter = 0\n",
        "\n",
        "\n",
        "G = Generator().cuda()\n",
        "D = Discriminator().cuda()\n",
        "\n",
        "def normal_init(m, mean=0.0, std=0.02):\n",
        "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
        "        m.weight.data.normal_(mean, std)\n",
        "\n",
        "G.apply(normal_init)\n",
        "D.apply(normal_init)\n",
        "\n",
        "opt_g = torch.optim.Adam(G.parameters(), lr=lr_g, betas=beta_g, eps=1e-08)\n",
        "opt_d = torch.optim.Adam(D.parameters(), lr=lr_d, betas=beta_d, eps=1e-08)\n",
        "\n",
        "losses_g, losses_d = [], []\n",
        "gen_imgs = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  if epoch in (12, 16):\n",
        "    opt_g.param_groups[0]['lr'] /= 10\n",
        "    opt_d.param_groups[0]['lr'] /= 10\n",
        "\n",
        "  for x, y in loader:\n",
        "    x = x.cuda()\n",
        "    y = one_hot(y).cuda()\n",
        "\n",
        "    loss_g, loss_d = train_on_batch(x, y, G, D, opt_g, opt_d, n_iter, update_gen_every)\n",
        "    losses_g.append(loss_g)\n",
        "    losses_d.append(loss_d)\n",
        "\n",
        "    n_iter += 1\n",
        "\n",
        "  print(f\"--- Epoch {epoch} ---\")\n",
        "  with torch.no_grad():\n",
        "    x_fake = G(fixed_noise, fixed_y).cpu()\n",
        "\n",
        "  gen_imgs.append(vutils.make_grid(x_fake, normalize=True, nrow=4).permute(1, 2, 0))\n",
        "\n",
        "  f, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "  axes[0].imshow(gen_imgs[-1])\n",
        "  axes[0].axis('off')\n",
        "  axes[0].set_title(\"Generated Images\")\n",
        "\n",
        "  indexes = np.arange(len(losses_g))\n",
        "  axes[1].plot(indexes, losses_g, label=\"Generator\")\n",
        "  axes[1].plot(indexes, losses_d, label=\"Discriminator\")\n",
        "  axes[1].set_title(\"Losses\")\n",
        "  axes[1].legend(loc=\"best\")\n",
        "\n",
        "  f.show()\n",
        "  plt.pause(0.5)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhT39ZSuWOZI"
      },
      "source": [
        "import imageio\n",
        "\n",
        "\n",
        "imageio.mimsave('mnist.gif', [(255 * img.cpu().numpy()).astype(np.uint8) for img in gen_imgs])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}