{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Autograd - Deepcourse",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3TS3x6pxBB0"
      },
      "source": [
        "<center><h1>Automatic Differentiation with Autograd</h1></center>\n",
        "\n",
        "<center><h2><a href=\"https://arthurdouillard.com/deepcourse/\">Course link</a></h2></center>\n",
        "\n",
        "To keep your modifications in case you want to come back later to this colab, do *File -> Save a copy in Drive*.\n",
        "\n",
        "If you find a mistake, or know how to improve this notebook, please open an issue [here](https://github.com/arthurdouillard/deepcourse/issues).\n",
        "\n",
        "In this notebook, we will see that we don't need to define the gradients of all our fancy operations. Pytorch's autograd can automatically find the gradients for us.\n",
        "\n",
        "After a few exercice on this, we will recode our MLP from [last course](https://colab.research.google.com/drive/1oP-uRpuxxUg5mbqz00IvGY6dcGV3mazm?usp=sharing) with the backward pass implemented with autograd. \n",
        "\n",
        "Finally, we will discover `torch.nn`, the package containing pre-implemented deep learning layers and how we can code a MLP using it.\n",
        "\n",
        "In the next code block, we are loading the data that will be useful later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB9o31WUpe9k"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digits = load_digits()\n",
        "X = torch.tensor(digits[\"images\"]).float()\n",
        "Y = torch.tensor(digits[\"target\"]).long()\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(X[42], cmap=\"gray\")\n",
        "plt.title(f\"Digit: {Y[42]}\");\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(X[64], cmap=\"gray\")\n",
        "plt.title(f\"Digit: {Y[64]}\");\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(X[1337], cmap=\"gray\")\n",
        "plt.title(f\"Digit: {Y[1337]}\");\n",
        "\n",
        "X = X / X.max()\n",
        "print(f\"Min and max value: {X.min()}, {X.max()}\")\n",
        "\n",
        "X = X.view(X.shape[0], -1)\n",
        "print(f\"Flatenned images shape: {X.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gp8MB_4pe9k"
      },
      "source": [
        "---\n",
        "\n",
        "# 1. Autograd tutorial\n",
        "\n",
        "Torch tensors have two optional attributes: `requires_grad`, and `grad`. The former says that we want the gradients to be computed for this given tensor, and the latter stores the actual gradients.\n",
        "\n",
        "By default `requires_grad` is false. When does it make sense to have it false? For example when:\n",
        "- the tensor is the images pixels. We don't want to optimize those;\n",
        "- we don't want to update the tensor/weights, because we want it *frozen* (more on it in later courses).\n",
        "\n",
        "Let's have a quick example, with the power of two function:\n",
        "\n",
        "$f(x) = x^2 \\rightarrow f'(x) = 2x$\n",
        "\n",
        "Thus $f(4) = 16$ and $f'(4) = 8$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj8tzaghpe9k"
      },
      "source": [
        "x = torch.tensor(4., requires_grad=True)\n",
        "\n",
        "def f(x):\n",
        "  return x ** 2\n",
        "\n",
        "y = f(x)\n",
        "print(f\"x={x}, f({x})={y}\")\n",
        "\n",
        "# We ask autograd to \"backward\" the gradients up to the start\n",
        "y.backward()\n",
        "\n",
        "print(f\"x={x}, f'({x})={x.grad}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD698wRg0aC4"
      },
      "source": [
        "Note that `backward` can, by default, only be called on a **scalar** (aka a number), not on a **tensor** (aka a vector, matrix, etc.). That's a specificity of pytorch.\n",
        "\n",
        "Here is two examples of how we can deal with that: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "newLoOBkpe9k"
      },
      "source": [
        "x = torch.tensor([1., 4., 2.], requires_grad=True)\n",
        "(x ** 2).sum().backward()\n",
        "print(f\"Gradients: {x.grad}\")\n",
        "\n",
        "x = torch.tensor([1., 4., 2.], requires_grad=True)\n",
        "(x ** 2).backward(torch.tensor([1, 1, 1]))\n",
        "print(f\"Gradients: {x.grad}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yijKe_DM2Z1A"
      },
      "source": [
        "Now let's take the formulas of a fully connected layer with 1 output...\n",
        "\n",
        "$\\mathbf{x} \\in \\mathbb{R^n}$, $\\mathbf{W} \\in \\mathbb{R^{n}}$, and $\\mathbf{b} \\in \\mathbb{R}$:\n",
        "\n",
        "$\\mathbf{h} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$\n",
        "\n",
        "... and code it in pytorch! What are gradients w.r.t the weights $\\mathbf{W}$ and $\\mathbf{b}$ according to you? Check it with autograd."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpKoK3TIwUsU"
      },
      "source": [
        "n = 2\n",
        "w = torch.randn(n, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "x = torch.randn(n)\n",
        "\n",
        "h = torch.dot(w, x) + b\n",
        "h.backward()\n",
        "\n",
        "print(f\"x is\\n {x}\")\n",
        "print(f\"grad W is\\n {w.grad}\")\n",
        "print(f\"grad b is\\n {b.grad}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xtDKHtJ6gQU"
      },
      "source": [
        "That's right, $\\nabla_\\mathbf{W} \\mathcal{L} = \\mathbf{x}$ and $\\nabla_\\mathbf{b} \\mathcal{L} = 1$. But what happens if our layer has not 1 outputs but $m$ with $\\mathbf{W} \\in \\mathbb{R^{m \\times n}}$, and $\\mathbf{b} \\in \\mathbb{R^{m}}$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzDXw9AP55Eg"
      },
      "source": [
        "n, m = 2, 3\n",
        "\n",
        "w = torch.randn(m, n, requires_grad=True)\n",
        "b = torch.randn(m, requires_grad=True)\n",
        "x = torch.randn(n)\n",
        "\n",
        "# torch.mv for \"Matrix-Vector product\"\n",
        "h = torch.mv(w, x) + b\n",
        "h.sum().backward()\n",
        "\n",
        "print(f\"x is\\n {x}\")\n",
        "print(f\"grad W is\\n {w.grad}\")\n",
        "print(f\"grad b is\\n {b.grad}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuxWKesPYVsg"
      },
      "source": [
        "$\\nabla_\\mathbf{W} \\mathcal{L}$ is simply $\\mathbf{x}$ stacked $m$ times. Which can do with an outer product: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlYzbk3LXzvE"
      },
      "source": [
        "torch.outer(torch.tensor([1., 1., 1.]), x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK3qDsUZYuKI"
      },
      "source": [
        "Intuitively, it means that $\\mathbf{x}$ contributes equally to each output dimension of the fully connected layer.\n",
        "\n",
        "Now, let's add a dimension to $\\mathbf{x} \\in \\mathbb{R^{k \\times n}}$ where $k$ represent the **batch size** (how many samples do we see in the same forward/backward pass). In doubt, look at the shape of each tensors to see how to do it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZv-HGAPWmwg"
      },
      "source": [
        "k, n, m = 4, 2, 3\n",
        "\n",
        "w = torch.randn(n, m, requires_grad=True)\n",
        "b = torch.randn(m, requires_grad=True)\n",
        "x = torch.randn(k, n)\n",
        "\n",
        "# torch.nmv for \"Matrix-Vector product\"\n",
        "h = torch.mm(x, w) + b\n",
        "print(f\"Output shape is {h.shape}\")\n",
        "h.sum().backward()\n",
        "\n",
        "print(f\"x is\\n {x}\")\n",
        "print(f\"grad W is\\n {w.grad}\")\n",
        "print(f\"grad b is\\n {b.grad}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvrEvYcHZU50"
      },
      "source": [
        "Notice that, now, we do a matrix multiplication between $\\mathbf{X}$ on the left, and $\\mathbf{W}$ on the right. Intuitively we don't want to change the batch size $k$ dimension of the inputs, but only the features dimension $n$.\n",
        "\n",
        "Now the values of $\\nabla_\\mathbf{W} \\mathcal{L}$ and $\\nabla_\\mathbf{b} \\mathcal{L}$ are dependent on the batch size $k$. Which is why we usually normalize both gradients by $k$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nOvlkJWXkuI"
      },
      "source": [
        "print(f\"Mean of x alongside the batch dimension (first one) is\\n {x.mean(dim=0)}\")\n",
        "print(f\"Normalized w grad is\\n {w.grad / k}\")\n",
        "print(f\"Normalized b grad is\\n {b.grad / k}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-N7o-1h3x6g"
      },
      "source": [
        "---\n",
        "\n",
        "# 2. MLP with Autograd\n",
        "\n",
        "Let's recode the MLP from previous session with autograd, we only have to modify the backward pass.\n",
        "\n",
        "Notice that we have wrapped the weights and biases into a `Parameter` class. By default it will force the wrapped tensor to requires the gradient among other stuffs.\n",
        "\n",
        "**Important**: when updating the weights and biases parameters, you have to modify their `data` attribute, not the actual tensor. Every operation done on a tensor that requires grad is recorded in order for Autograd to compute the necessary gradients. But we don't need that when updating the weights and biases.\n",
        "\n",
        "```python\n",
        "new_w = old_w - lr * gradient  # Bad\n",
        "\n",
        "new_w.data = old_w.data - lr * gradient  # Goo``\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0HOkht3eTaM"
      },
      "source": [
        "from torch.nn import Parameter\n",
        "\n",
        "def softmax(x):\n",
        "    maximum_value = x.max(dim=1, keepdims=True)[0]\n",
        "    e = torch.exp(x - maximum_value)\n",
        "    return e / e.sum(dim=1, keepdims=True)\n",
        "\n",
        "def cross_entropy(probs, targets):\n",
        "    return -torch.sum(targets * torch.log(probs + 1e-8), dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsCQhtWPpe9k"
      },
      "source": [
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size, nb_classes, learning_rate=0.01):\n",
        "        self.w_hidden = Parameter(torch.randn(input_size, hidden_size).float())\n",
        "        self.b_hidden = Parameter(torch.zeros(hidden_size).float())\n",
        "        \n",
        "        self.w_output = Parameter(torch.randn(hidden_size, nb_classes).float())\n",
        "        self.b_output = Parameter(torch.zeros(nb_classes).float())\n",
        "        \n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Use torch.mm and torch.tanh\n",
        "        h_tilde = # TODO\n",
        "        h = # TODO\n",
        "        logits = # TODO\n",
        "        \n",
        "        return logits, h_tilde, h\n",
        "        \n",
        "    def fit(self, inputs, targets, train=True):\n",
        "        logits, *outputs = self.forward(inputs)\n",
        "        probs = softmax(logits)\n",
        "        loss = cross_entropy(probs, torch.eye(10)[targets]).mean()\n",
        "        if train:\n",
        "            self.backward(inputs, probs, targets, loss, *outputs)\n",
        "        return loss\n",
        "    \n",
        "    def backward(self, inputs, probs, targets, loss, h_tilde, h):\n",
        "        batch_size = len(probs)\n",
        "        \n",
        "        loss.backward()  # Fill the graph with .grad attributes\n",
        "\n",
        "        # TODO, update all learnable parameters\n",
        "        \n",
        "    def accuracy(self, inputs, targets):\n",
        "        y_pred = self.forward(inputs)[0].argmax(dim=1)\n",
        "        y_true = targets\n",
        "        \n",
        "        return torch.mean((y_pred == y_true).float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmR9j8iNku30"
      },
      "source": [
        "# Execute this cell to see the solution, but try to do it by yourself before!\n",
        "!wget https://raw.githubusercontent.com/arthurdouillard/deepcourse/master/static/code/backpropagation/mlp_autograd.py\n",
        "%pycat mlp_autograd.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pntveIepe9k"
      },
      "source": [
        "model = MLP(X.shape[1], 50, len(torch.unique(Y)), 0.1)\n",
        "model.accuracy(X, Y)\n",
        "\n",
        "batch_size = 32\n",
        "nb_epochs = 10\n",
        "\n",
        "epochs, accuracies, losses = [], [], []\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    for batch_index in range(0, len(X), batch_size):\n",
        "        batch_X = X[batch_index:batch_index + batch_size]\n",
        "        batch_Y = Y[batch_index:batch_index + batch_size]\n",
        "    \n",
        "        model.fit(batch_X, batch_Y)\n",
        "        \n",
        "    loss = model.fit(X, Y, train=False)\n",
        "    acc = model.accuracy(X, Y)\n",
        "    \n",
        "    print(f\"Epoch: {epoch}, loss: {loss}, accuracy: {acc}\")\n",
        "    epochs.append(epoch)\n",
        "    losses.append(loss)\n",
        "    accuracies.append(acc)\n",
        "    \n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, accuracies)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"accuracy\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RfrsA_qeIX8"
      },
      "source": [
        "---\n",
        "\n",
        "# 3. MLP with `torch.nn`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAMDWiBmpe9k"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, nb_classes):\n",
        "    super().__init__()  # Important! torch initializes a bunch of stuff with this line\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size, bias=True)\n",
        "    self.fc2 = nn.Linear(hidden_size, nb_classes, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h = torch.tanh(self.fc1(x))\n",
        "    return self.fc2(h)\n",
        "\n",
        "\n",
        "def compute_accuracy(y_pred, y_true):\n",
        "  return torch.mean((y_pred == y_true).float())\n",
        "\n",
        "\n",
        "model = MLP(X.shape[1], 50, len(torch.unique(Y)))\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "batch_size = 32\n",
        "nb_epochs = 10\n",
        "\n",
        "epochs, accuracies, losses = [], [], []\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "    for batch_index in range(0, len(X), batch_size):\n",
        "        batch_X = X[batch_index:batch_index + batch_size]\n",
        "        batch_Y = Y[batch_index:batch_index + batch_size]\n",
        "    \n",
        "        optimizer.zero_grad()\n",
        "        logits = model(batch_X)\n",
        "        loss = F.cross_entropy(logits, batch_Y)  # Does implicitely softmax+CE\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    \n",
        "    model.eval()\n",
        "    logits = model(X)\n",
        "    loss = F.cross_entropy(logits, Y)\n",
        "    acc = compute_accuracy(logits.argmax(dim=1), Y)\n",
        "    model.train()\n",
        "    \n",
        "    print(f\"Epoch: {epoch}, loss: {loss}, accuracy: {acc}\")\n",
        "    epochs.append(epoch)\n",
        "    losses.append(loss)\n",
        "    accuracies.append(acc)\n",
        "    \n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, accuracies)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"accuracy\");\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4Ovs3oklVQE"
      },
      "source": [
        "Try by yourself to extend this network and see the results:\n",
        "- add more layers\n",
        "- change the hidden dimension size\n",
        "- change the non-linearity "
      ]
    }
  ]
}